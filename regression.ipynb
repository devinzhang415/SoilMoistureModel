{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get data\n",
    "\"\"\"\n",
    "def loader(dataset_locs: str | list[str]) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load dataset\n",
    "    Args:\n",
    "        dataset_locs: Location of the CSV file containing information regarding the dataset.\n",
    "    Return:\n",
    "        X: (n, 4) data nparray with columns average red, average green, average blue, and lux in moisture\n",
    "        y: (n,) label nparray with column moisture\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "\n",
    "    transform = transforms.ToTensor()\n",
    "\n",
    "    if isinstance(dataset_locs, str):\n",
    "        dataset_locs = [dataset_locs]\n",
    "    for dataset_loc in dataset_locs:\n",
    "        dataset = pd.read_csv(dataset_loc)\n",
    "\n",
    "        paths = dataset.iloc[:, 2] # Get image paths\n",
    "        moistures = dataset.iloc[:, 3] # Get moisture levels\n",
    "        luxs = dataset.iloc[:, 4] # Get lux levels\n",
    "\n",
    "        # Get data\n",
    "        n = len(paths)\n",
    "        for i in range(n):\n",
    "            path = paths[i]\n",
    "            moitsure = moistures[i]\n",
    "            lux = luxs[i]\n",
    "\n",
    "            # Get image\n",
    "            if not os.path.exists(path):\n",
    "                continue\n",
    "            image = Image.open(path)\n",
    "\n",
    "            # Crop image to center 120x120 px\n",
    "            height, width = image.size\n",
    "            left = (width - 120) / 2\n",
    "            top = (height - 120) / 2\n",
    "            right = (width + 120) / 2\n",
    "            bottom = (height + 120) / 2\n",
    "            image = image.crop((left, top, right, bottom))\n",
    "\n",
    "            # Get average RGB\n",
    "            image_tensor = transform(image)\n",
    "            avg_rgb = (torch.mean(image_tensor, dim=(1, 2)) * 255).numpy()\n",
    "            avg_r, avg_g, avg_b = avg_rgb\n",
    "            \n",
    "            datapoint = [avg_r, avg_g, avg_b, lux]\n",
    "            X.append(datapoint)\n",
    "            y.append(moitsure)\n",
    "    \n",
    "    X, y = np.array(X), np.array(y)\n",
    "    return X, y\n",
    "\n",
    "# paths = ['Data_i11_ds/dataset_i11_ds.csv']\n",
    "# paths = ['Data_i11_is/dataset_i11_is.csv']\n",
    "paths = ['Data_i11_ds/dataset_i11_ds.csv', 'Data_i11_is/dataset_i11_is.csv']\n",
    "# paths = ['Data_fpbicc/Dataset_fpbicc_filtered.csv']\n",
    "X, y = loader(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Preprocess data\n",
    "\"\"\"\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "def preprocess(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Preprocess the data and labels.\n",
    "    \"\"\"\n",
    "    y_train = y_train.reshape(-1, 1)\n",
    "    y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "    X_train = scaler_X.fit_transform(X_train)\n",
    "    X_test = scaler_X.transform(X_test)\n",
    "    y_train = scaler_y.fit_transform(y_train)\n",
    "    y_test = scaler_y.transform(y_test)\n",
    "\n",
    "    return X_train, X_test, y_train.ravel(), y_test.ravel()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train, X_test, y_train, y_test = preprocess(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "ax1.scatter(y_train, X_train[:, 0], c='red')\n",
    "ax1.set_xlabel(\"avg r\")\n",
    "ax1.set_ylabel(\"moisture\")\n",
    "ax2.scatter(y_train, X_train[:, 1], c='green')\n",
    "ax2.set_xlabel(\"avg g\")\n",
    "ax3.scatter(X_train[:, 2], , c='blue')\n",
    "ax3.set_xlabel(\"avg b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SVR model\n",
    "\"\"\"\n",
    "svr = SVR(kernel='rbf')\n",
    "svr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SVR test\n",
    "\"\"\"\n",
    "y_pred = svr.predict(X_test)\n",
    "\n",
    "r2 = svr.score(X_test, y_test)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print('SVR r2: {}'.format(r2))\n",
    "print('SVR Root Mean Squared Error: {}'.format(rmse))\n",
    "\n",
    "# n = y_pred.shape[0]\n",
    "# for i in range(n):\n",
    "#     pred = y_pred[i]\n",
    "#     actual = y_test[i]\n",
    "#     print(f'Pred: {pred}, Actual: {actual}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Soil-Moisture-Model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
